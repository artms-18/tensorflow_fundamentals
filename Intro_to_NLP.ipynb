{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRnowaF7z+k4WfQ/Q8/Uwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artms-18/tensorflow_fundamentals/blob/main/Intro_to_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2YYd-dzOCzO"
      },
      "source": [
        "# Introduction to NLP Fundamentals in Tensorflow\n",
        "\n",
        "NLP has the goal of deriving information out of natural language (could be sequences text or speech).\n",
        "\n",
        "Another common term for NLP problems is sequence to sequence problems (seq2seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwL6eAuxOez1"
      },
      "source": [
        "## Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NmbUEodOibH",
        "outputId": "f1186966-e65a-420e-e748-72c58a6424c0"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-f6e8450b-eef6-58c7-f161-44159647d129)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXRui5LaOmC1"
      },
      "source": [
        "## Get helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EKDbxTLOta0",
        "outputId": "af24b259-5709-4198-ab3a-2e04c8fbef35"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import a series of helper function for the notebook\n",
        "\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-14 12:14:10--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-14 12:14:10 (74.4 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFOGPIupPDFD"
      },
      "source": [
        "## Get a text dataaset\n",
        "\n",
        "The dataset we're going to be using is Kaggle's introduction to NLP dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n07wcR_vPax1",
        "outputId": "a5713077-6ad2-43bc-f1ea-8cd536b09912"
      },
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "\n",
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-14 12:14:13--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 74.125.140.128, 108.177.15.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2021-05-14 12:14:13 (74.3 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ncZHW9NfqH"
      },
      "source": [
        "## Visualizing a text dataset\n",
        "\n",
        "To visualie our text samples, we first have to read them in, one way to do so would be to use Python: \n",
        "\n",
        "Another way to do this is to use pandas.\n",
        "\n",
        "(If text dataset is very large, cannot ise pandas (will tkaes of to much RAM).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "pMSX-pP6N4ZP",
        "outputId": "bb7e96ab-3dc6-4417-82cc-270b9013d292"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6wEgvLPWOq38",
        "outputId": "54c55786-c31b-46d8-c0af-9cd954748a9e"
      },
      "source": [
        "train_df['text'][0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GMJJNtQQOzCx",
        "outputId": "ff94a77d-4810-42db-9138-ef4960990a8d"
      },
      "source": [
        "# Shuffle training dataframe\n",
        "\n",
        "train_df_shuffled = train_df.sample(frac = 1, random_state = 42)\n",
        "train_df_shuffled.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... target\n",
              "2644  3796  ...      1\n",
              "2227  3185  ...      0\n",
              "5448  7769  ...      1\n",
              "132    191  ...      0\n",
              "6845  9810  ...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Wf5tQuYPPOI5",
        "outputId": "06194359-e275-47a1-a091-f954218cfa78"
      },
      "source": [
        "# What does the test dataframe look like?\n",
        "\n",
        "test_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85kR-NvnPYL5",
        "outputId": "c3b814d2-3167-4613-90cd-c0021b2c03f1"
      },
      "source": [
        "# How many examples of each class?\n",
        "train_df.target.value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNxPDFVsPjaE",
        "outputId": "2373d594-2998-4f8f-ee57-8db6d24efb57"
      },
      "source": [
        "# How many total samples?\n",
        "len(train_df), len(test_df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 3263)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQBONc5XQB21",
        "outputId": "dc807377-1f73-4122-f378-ab5cde2f2b45"
      },
      "source": [
        "# Let's visualize some random training examples\n",
        "\n",
        "import random\n",
        "\n",
        "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
        "\n",
        "for row in train_df_shuffled[['text', 'target']][random_index: random_index + 5].itertuples():\n",
        "  _, text, target = row #gets rid of index\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target: 1 (real disaster)\n",
            "Text:\n",
            "EXCLUSIVE: In 179 fatalities involving on-duty NYPD cops in 15 years only 3åÊÛ_ http://t.co/Cn1joMMUGH\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "Text:\n",
            "At the #999day Emergency Services display! Meeting the brave police firefighters and first responders that help keep us safe! ????????\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster\n",
            "Text:\n",
            "Progressive greetings!\n",
            "\n",
            "In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "Text:\n",
            "Swiss Kosher Hotel Bridge Collapse Injures Five People - http://t.co/TxIestoX5n @JewishPress\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster\n",
            "Text:\n",
            "Get a load of this welfare loving sponge....a blight on society. http://t.co/rrZbZGO48N\n",
            "\n",
            "---\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOA44HYRFO4"
      },
      "source": [
        "### Split data into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLvaFplXVxYB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T--31tdmV8-p"
      },
      "source": [
        "# Use train_test_split top split training data into training and validation sets\n",
        "\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled['text'].to_numpy(), train_df_shuffled['target'].to_numpy(),\n",
        "                                                                            test_size = 0.1, random_state = 42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMmCJhagWgrX",
        "outputId": "2554d47b-e3ba-4d20-c33b-0609722d67cb"
      },
      "source": [
        "# Check the lengths\n",
        "\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1IAOaMvWn1h",
        "outputId": "a91a0c3a-5bbf-4bf8-b4f2-18154acfb77c"
      },
      "source": [
        "# Check the first 10 samples\n",
        "\n",
        "train_sentences[:10], train_labels[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBPYFbfpWxka"
      },
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "When dealing with a text problem, one of the first things you'll have to do before you can build a midek is to convert your text to numbers.\n",
        "\n",
        "There are a few ways to do this, namely:\n",
        "* Tokenization - direct mapping of  token( a toke could be a word or a character) to number\n",
        "* embedding - create a matrix of feature vectors for each token the size of the feature vector can be defined and this embedding can be learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F418KsbgeMiz"
      },
      "source": [
        "### Text vectorization (tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDCHzgPeRPi",
        "outputId": "34a13947-d072-4f2b-cacb-0655906728cc"
      },
      "source": [
        "train_sentences[:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "       'Imagine getting flattened by Kurt Zouma',\n",
              "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JMd2f1QeSkz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Use the default TextVectorization parameters\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens = None, # how many words in the vocab (automatically add <OOV>)\n",
        "                                    standardize = \"lower_and_strip_punctuation\",\n",
        "                                    split = \"whitespace\", \n",
        "                                    ngrams = None,\n",
        "                                    output_mode = \"int\", #how to map tokens to numbers\n",
        "                                    output_sequence_length = None, #pads each sequence to the logest sequence\n",
        "                                    pad_to_max_tokens = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2DSXz93fNcV",
        "outputId": "79011d29-c030-41b7-a8fc-6a3c1d104ac9"
      },
      "source": [
        "# Find the average number of tokens (words) in the training tweets\n",
        "\n",
        "round(sum(len(i.split()) for i in train_sentences)) / len(train_sentences)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.901036345059115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PyZrrwPhe4U"
      },
      "source": [
        "# Setup text vectorization variables\n",
        "\n",
        "max_vocab_length = 10000 #max num of words to have in our vocabulary\n",
        "max_length = 15 # max length our seuqences will be (how many words in a tweet, amodel would see)\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens = max_vocab_length, output_mode= 'int', output_sequence_length = max_length)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDVzTznLiDwm"
      },
      "source": [
        "#fit the text vectorier to the training text\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKIukLPgmz5A",
        "outputId": "1507ec94-a283-4e27-952e-b578ddd1af8f"
      },
      "source": [
        "# Create a sample sentence and tokenize it\n",
        "\n",
        "sample_sentence = \"Theres a flood in my street!\"\n",
        "\n",
        "text_vectorizer([sample_sentence])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Td3xFgCnGfD",
        "outputId": "d983cb8f-40a2-4deb-c0a1-06754ea8139e"
      },
      "source": [
        "# Choose a random sentence from the training dataset and tokenize iot\n",
        "\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n {random_sentence}\\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:\n",
            " I liked a @YouTube video http://t.co/9Vw0uQQi1y Marvel VS DC (Avengers Battle!)\n",
            "\n",
            "Vectorized version:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[   8,  317,    3,  114,   72,    1, 3652,  975, 1306, 6152,  442,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjEI87-Mnpiw",
        "outputId": "67fc2e6b-8292-47c0-aa6a-c9be326658ac"
      },
      "source": [
        "# Get the unique words in the vocabulary\n",
        "\n",
        "words_in_vocab = text_vectorizer.get_vocabulary() #get all of the unique words in the training data\n",
        "top_5_words = words_in_vocab[:5] # get the most common words\n",
        "bottom_5_words = words_in_vocab[-5:] #get the least common words\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"5 most comon words: {top_5_words}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in vocab: 10000\n",
            "5 most comon words: ['', '[UNK]', 'the', 'a', 'in']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWW769H8pF-8"
      },
      "source": [
        "### Creating an Embedding using an Embedding Layer\n",
        "\n",
        "To make our embedding, we're going to use tensoflows embedding layer\n",
        "\n",
        "The parameters we actually care about:\n",
        "* `input_dim` = the size of our vocabulary\n",
        "* `output_dim` = the size of the output embedding vector, for example, a value of 100 would mean each token gets represented by a vecotr 100 long\n",
        "* `input_length` = length of the sequences being passed to the embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B7iSg3Ep7iT",
        "outputId": "f216b001-e611-4dbd-cca6-4e5c5339b1eb"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# in keras, wordks best when numbers are divisible by eight\n",
        "\n",
        "embedding = layers.Embedding(input_dim = max_vocab_length,\n",
        "                             output_dim = 128,\n",
        "                             input_length = max_length\n",
        "                             )\n",
        "embedding"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fea6689af50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbX5XRVfrNRv",
        "outputId": "d38647e5-b9b7-4661-b617-ebcd7d9bbee9"
      },
      "source": [
        "# Get a random sentence from the training set\n",
        "\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n {random_sentence}\\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn into dense vecotrs of fixed size)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:\n",
            " This bowl got me thinking... Damn I've been blazing for so damn long\n",
            "\n",
            "Embedded version:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.01416222, -0.00747452,  0.02948919, ...,  0.00822681,\n",
              "          0.03080311,  0.04567153],\n",
              "        [-0.048515  ,  0.0095826 ,  0.01933989, ..., -0.03448155,\n",
              "          0.02707541,  0.020882  ],\n",
              "        [-0.02473872,  0.00254079, -0.01220051, ...,  0.01977875,\n",
              "         -0.00135307, -0.04702128],\n",
              "        ...,\n",
              "        [-0.03091977, -0.04617801, -0.0321962 , ..., -0.00742856,\n",
              "          0.02181156,  0.04397157],\n",
              "        [ 0.04249508,  0.0063271 ,  0.00146083, ...,  0.03756789,\n",
              "         -0.00451548,  0.04296427],\n",
              "        [ 0.04249508,  0.0063271 ,  0.00146083, ...,  0.03756789,\n",
              "         -0.00451548,  0.04296427]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atTvotyfryMA",
        "outputId": "eba980dc-e498-4f16-800a-f3ed049691cd"
      },
      "source": [
        "# Chekc out a single token's embedding\n",
        "\n",
        "sample_embed[0][0], sample_embed[0][0].shape, random_sentence[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([ 0.01416222, -0.00747452,  0.02948919, -0.02067909,  0.02454457,\n",
              "         0.00482122, -0.04081089, -0.01009145,  0.03603034, -0.04845396,\n",
              "         0.03969827,  0.03483386,  0.03346867, -0.01026805,  0.04684628,\n",
              "         0.04672552,  0.00194039, -0.00422686,  0.04563434,  0.0056396 ,\n",
              "         0.00313479,  0.0365626 ,  0.01218349,  0.01833708,  0.04870036,\n",
              "         0.0239489 , -0.02445322,  0.00913516,  0.04304917, -0.00791363,\n",
              "        -0.0075011 , -0.01526998, -0.02009779,  0.00711288, -0.00983906,\n",
              "        -0.02713701,  0.04935445, -0.01496279,  0.02390981,  0.02179307,\n",
              "         0.01898582, -0.02114235, -0.00566342, -0.03910711, -0.0327425 ,\n",
              "         0.01297854, -0.03582448, -0.04305572,  0.00250013, -0.03314399,\n",
              "        -0.03390209, -0.00513138, -0.01527756,  0.00566555, -0.03678872,\n",
              "         0.01790806,  0.00865363,  0.0325066 , -0.00673614, -0.03922405,\n",
              "        -0.01352591, -0.01498772, -0.0003363 ,  0.01690963, -0.00935229,\n",
              "        -0.00907559,  0.0030308 ,  0.00508468,  0.03616209,  0.0219585 ,\n",
              "         0.04195459,  0.01627186, -0.01551858, -0.03133081,  0.02927228,\n",
              "         0.04947761, -0.03321185,  0.02570827,  0.0429222 , -0.00735293,\n",
              "        -0.01860281, -0.01748028, -0.02724524,  0.01310474, -0.03906358,\n",
              "         0.04775027, -0.01780312,  0.02251444, -0.01018643,  0.04588738,\n",
              "        -0.04832952,  0.01589571,  0.00955274,  0.01137489,  0.02386618,\n",
              "         0.01377111,  0.04153209,  0.02987224,  0.00235401, -0.02119339,\n",
              "         0.00802694,  0.04669187,  0.04939539,  0.0205708 ,  0.02734847,\n",
              "         0.03867843, -0.00820624,  0.0400779 , -0.00247858, -0.02532439,\n",
              "        -0.0207955 , -0.01759204, -0.02454149,  0.00802558,  0.00447001,\n",
              "        -0.01514932, -0.01939358, -0.01976721, -0.02843105, -0.04763773,\n",
              "        -0.00538392,  0.003721  ,  0.04136154, -0.01331608, -0.03299902,\n",
              "         0.00822681,  0.03080311,  0.04567153], dtype=float32)>,\n",
              " TensorShape([128]),\n",
              " 'T')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtMpeo5YsY9x"
      },
      "source": [
        "## Modelling a text dataset (running a series of experiments)\n",
        "\n",
        "Now we're got a wat to turn our text srequences into numbers, it's time to start building a series of modelling experiments, start from a baseline and move on from there \n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neual network (dense model)\n",
        "* Model 2: GRU model (RNN)\n",
        "* Model 4: Bidirectional-LSTM\n",
        "* Model 5: 1d Convolutional Neural Network (CNN)\n",
        "* Model 6: Tensorflow Hub Pretrained Geature Extractor (using transfer learning for NLP)\n",
        "* Model 7: Same as model 6 with 10% of training data\n",
        "\n",
        "How are we going to approach all of these?\n",
        "\n",
        "Use the standard steps in modelling with tensorflow:\n",
        "\n",
        "* Create a model\n",
        "* Build a model\n",
        "* Compile a model\n",
        "* Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRqK9DU2zyOB"
      },
      "source": [
        "### Model 0: Getting a baseline\n",
        "\n",
        "As with all machine learning modelling experiments it's important to create a baseline model so u got a benchmark for future experiments to build upon\n",
        "\n",
        "To create our baseline, we'll use SKlearns Multinomial Naive Bayes using the TF-IDF formula to convert our words to numbers\n",
        "\n",
        "> **Note:** It's common practive to use non-DL algorithms as a baseline because of their speed, then later using DL to improve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHA03tu11fw",
        "outputId": "e82fb0ab-29ae-4b42-d7b2-c2af21b555a3"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()),\n",
        "                    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "\n",
        "model_0.fit(train_sentences, train_labels)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0pGO_Cu2nOz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}